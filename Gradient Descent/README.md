📉 Gradient Descent from Scratch
A hands-on and visual deep dive into one of the most essential optimization algorithms in machine learning — Gradient Descent, built using just Python and NumPy.

🧠 What You'll Learn
✅ Manual implementation of gradient descent
✅ Deriving gradients from scratch
✅ Linear regression using only NumPy
✅ Visualizing training and convergence

📸 Screenshots & Results
🔁 Training Progress — Loss Over Iterations
Shows how the Mean Squared Error reduces over time as the model learns.
<img width="1372" height="499" alt="Loss Curve" src="https://github.com/user-attachments/assets/a7630528-0b2a-456c-967a-b397b57be13e" />

📈 Final Regression Line
The line fitted to the data points using learned weights and bias.
<img width="1372" height="499" alt="Regression Line" src="https://github.com/user-attachments/assets/8eee552c-af8f-4bbb-99fd-de130b7a8c3e" />

🧮 Sample Output Printout
Model output showing final values of loss, weights, and bias after training.
<img width="1396" height="388" alt="Console Output" src="https://github.com/user-attachments/assets/699ea094-3522-4c03-962f-15d71142ab35" />

📁 Project Structure
bash
Copy
Edit
📦 GradientDescent
 ┣ 📜 GradientDescent.ipynb    # Main notebook with logic & plots
 ┗ 📄 README.md                 # This file
⚙️ Tech Stack
🐍 Python 3.x

🧮 NumPy

📊 Matplotlib

📓 Jupyter Notebook

🚀 How to Run
bash
Copy
Edit
git clone https://github.com/Abdulwahab-ml/GradientDescent.git
cd GradientDescent
jupyter notebook GradientDescent.ipynb
💡 Ideas to Extend
Add multiple feature support

Experiment with different learning rates

Compare with scikit-learn for validation

👤 Author
Abdul Wahab
🌱 Learning Machine Learning from scratch
🔗 @abdulwahab-ml
