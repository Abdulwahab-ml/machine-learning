📉 Gradient Descent from Scratch
Welcome to the Gradient Descent project, where we implement and visualize one of the most fundamental optimization algorithms used in machine learning — built entirely from scratch using Python and NumPy.

🚀 Project Overview
This notebook demonstrates:

Intuitive understanding of gradient descent

Manual derivation and implementation of cost gradients

Training a simple linear regression model using custom gradient descent

Visualization of the loss over iterations

📁 File Structure
python
Copy
Edit
📦 GradientDescent
 ┣ 📜 GradientDescent.ipynb    ← Main notebook with all the logic & visualizations
 ┗ 📄 README.md                 ← You're reading it now!
🛠️ Technologies Used
Python 3.x

NumPy

Matplotlib (for plotting loss and regression lines)

Jupyter Notebook

📊 Output Sample
Loss curve (MSE vs Iterations)

Linear regression plot fitted by gradient descent

Print statements showing convergence progress

✅ How to Run
Clone the repository:

bash
Copy
Edit
git clone https://github.com/yourusername/GradientDescent.git
cd GradientDescent
Open the notebook:

bash
Copy
Edit
jupyter notebook GradientDescent.ipynb
Run all cells and observe the model training and visualizations.

🧠 Key Learnings
How gradient descent works step-by-step

Importance of learning rate and convergence

Relationship between gradients and cost minimization

🤝 Contributing
Contributions are welcome! If you spot improvements or want to extend the notebook (e.g., adding regularization, support for multiple variables), feel free to fork and submit a PR.

📬 Contact
Abdul Wahab
Beginner in Machine Learning | Exploring core ML concepts
📫 GitHub: abdulwahab-ml
